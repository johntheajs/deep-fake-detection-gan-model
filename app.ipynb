{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140002 files belonging to 2 classes.\n",
      "Found 39428 files belonging to 2 classes.\n",
      "Found 10905 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from Google Drive\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'D:\\Projects\\CDP\\Dataset\\Dataset-1\\Train',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    image_size=(28, 28),\n",
    "    batch_size=32,\n",
    "    color_mode=\"grayscale\"\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'D:\\Projects\\CDP\\Dataset\\Dataset-1\\Validation',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    image_size=(28, 28),\n",
    "    batch_size=32,\n",
    "    color_mode=\"grayscale\"\n",
    ")\n",
    "\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'D:\\Projects\\CDP\\Dataset\\Dataset-1\\Test',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    image_size=(28, 28),\n",
    "    batch_size=32,\n",
    "    color_mode=\"grayscale\"\n",
    ")\n",
    "\n",
    "# Normalize the pixel values to [-1, 1] for GAN training\n",
    "def normalize(image, label):\n",
    "    image = (image - 127.5) / 127.5\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(normalize)\n",
    "val_dataset = val_dataset.map(normalize)\n",
    "test_dataset = test_dataset.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CGAN Generator Model (with label input)\n",
    "def make_generator_model():\n",
    "    # Label input and embedding\n",
    "    label_input = layers.Input(shape=(1,))\n",
    "    noise_input = layers.Input(shape=(100,))\n",
    "\n",
    "    # Embed and reshape the label\n",
    "    label_embedding = layers.Embedding(2, 50)(label_input)\n",
    "    label_dense = layers.Dense(7 * 7 * 1)(label_embedding)\n",
    "    label_reshape = layers.Reshape((7, 7, 1))(label_dense)\n",
    "\n",
    "    # Noise input and reshape\n",
    "    noise_dense = layers.Dense(7 * 7 * 256, use_bias=False)(noise_input)\n",
    "    noise_batchnorm = layers.BatchNormalization()(noise_dense)\n",
    "    noise_reshape = layers.Reshape((7, 7, 256))(noise_batchnorm)\n",
    "\n",
    "    # Concatenate label and noise\n",
    "    concatenated = layers.Concatenate()([noise_reshape, label_reshape])\n",
    "\n",
    "    # Building the model using the functional API\n",
    "    x = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False)(concatenated)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    output = layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh')(x)\n",
    "\n",
    "    return tf.keras.Model([noise_input, label_input], output)\n",
    "\n",
    "def make_discriminator_model():\n",
    "    # Label input and embedding\n",
    "    label_input = layers.Input(shape=(1,))\n",
    "    image_input = layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "    # Embed and reshape the label\n",
    "    label_embedding = layers.Embedding(2, 50)(label_input)\n",
    "    label_dense = layers.Dense(28 * 28)(label_embedding)\n",
    "    label_reshape = layers.Reshape((28, 28, 1))(label_dense)\n",
    "\n",
    "    # Concatenate label and image input\n",
    "    concatenated = layers.Concatenate()([image_input, label_reshape])\n",
    "\n",
    "    # Build the discriminator model using the functional API\n",
    "    x = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(concatenated)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "\n",
    "    return tf.keras.Model([image_input, label_input], output)\n",
    "\n",
    "\n",
    "# Build the generator and discriminator\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# Loss and optimizers\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# Training Loop\n",
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "BATCH_SIZE = 32\n",
    "num_examples_to_generate = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 1086.8935239315033 sec\n",
      "Time for epoch 2 is 724.2298266887665 sec\n",
      "Time for epoch 3 is 660.2270333766937 sec\n",
      "Time for epoch 4 is 584.8917462825775 sec\n",
      "Time for epoch 5 is 767.5784523487091 sec\n",
      "Time for epoch 6 is 604.1639895439148 sec\n",
      "Time for epoch 7 is 595.9945447444916 sec\n",
      "Time for epoch 8 is 592.8673405647278 sec\n",
      "Time for epoch 9 is 591.1071863174438 sec\n",
      "Time for epoch 10 is 592.9089303016663 sec\n",
      "Time for epoch 11 is 598.4381096363068 sec\n",
      "Time for epoch 12 is 597.5404062271118 sec\n",
      "Time for epoch 13 is 591.3100609779358 sec\n",
      "Time for epoch 14 is 594.1696751117706 sec\n",
      "Time for epoch 15 is 589.2087919712067 sec\n",
      "Time for epoch 16 is 589.8747265338898 sec\n",
      "Time for epoch 17 is 589.4686923027039 sec\n",
      "Time for epoch 18 is 592.6251075267792 sec\n",
      "Time for epoch 19 is 587.7482159137726 sec\n",
      "Time for epoch 20 is 587.30588722229 sec\n",
      "Time for epoch 21 is 591.4061667919159 sec\n",
      "Time for epoch 22 is 623.9222757816315 sec\n",
      "Time for epoch 23 is 619.6030957698822 sec\n",
      "Time for epoch 24 is 610.4313173294067 sec\n",
      "Time for epoch 25 is 628.5604839324951 sec\n",
      "Time for epoch 26 is 582.3560996055603 sec\n",
      "Time for epoch 27 is 600.3908922672272 sec\n",
      "Time for epoch 28 is 632.5226175785065 sec\n",
      "Time for epoch 29 is 737.7335779666901 sec\n",
      "Time for epoch 30 is 593.27459025383 sec\n",
      "Time for epoch 31 is 591.3468551635742 sec\n",
      "Time for epoch 32 is 576.0847270488739 sec\n",
      "Time for epoch 33 is 581.0281934738159 sec\n",
      "Time for epoch 34 is 579.9729478359222 sec\n",
      "Time for epoch 35 is 583.7611107826233 sec\n",
      "Time for epoch 36 is 579.7570490837097 sec\n",
      "Time for epoch 37 is 586.3091335296631 sec\n",
      "Time for epoch 38 is 587.0399651527405 sec\n",
      "Time for epoch 39 is 598.5937571525574 sec\n",
      "Time for epoch 40 is 589.305392742157 sec\n",
      "Time for epoch 41 is 584.0908155441284 sec\n",
      "Time for epoch 42 is 578.5652825832367 sec\n",
      "Time for epoch 43 is 587.9012184143066 sec\n",
      "Time for epoch 44 is 593.3419873714447 sec\n",
      "Time for epoch 45 is 589.9437296390533 sec\n",
      "Time for epoch 46 is 590.8707242012024 sec\n",
      "Time for epoch 47 is 590.9081904888153 sec\n",
      "Time for epoch 48 is 589.4470171928406 sec\n",
      "Time for epoch 49 is 592.2673740386963 sec\n",
      "Time for epoch 50 is 592.6575036048889 sec\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    noise = tf.random.normal([images.shape[0], noise_dim])  # Match batch size with images\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generate images using the generator\n",
    "        generated_images = generator([noise, labels], training=True)\n",
    "        \n",
    "        # Get real and fake output from the discriminator\n",
    "        real_output = discriminator([images, labels], training=True)\n",
    "        fake_output = discriminator([generated_images, labels], training=True)\n",
    "        \n",
    "        # Calculate generator and discriminator losses\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    # Apply gradients to the generator and discriminator\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for image_batch, labels_batch in dataset:\n",
    "            train_step(image_batch, labels_batch)\n",
    "\n",
    "        print(f\"Time for epoch {epoch + 1} is {time.time() - start} sec\")\n",
    "        \n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10353   552]\n",
      " [ 5171  5734]]\n",
      "Accuracy: 0.7375974323704723\n",
      "Precision: 0.9121858097359211\n",
      "Recall: 0.5258138468592389\n",
      "F1-Score: 0.6670932464661742\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Metrics\n",
    "\n",
    "\n",
    "# Evaluation Metrics\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for test_images, test_labels in test_dataset:\n",
    "    real_output = discriminator([test_images, test_labels], training=False)\n",
    "    fake_images = generator([tf.random.normal([len(test_images), noise_dim]), test_labels], training=False)\n",
    "    fake_output = discriminator([fake_images, test_labels], training=False)\n",
    "\n",
    "    y_true.extend([1] * len(test_images) + [0] * len(fake_images))\n",
    "    y_pred.extend([1 if p > 0 else 0 for p in real_output] + [1 if p > 0 else 0 for p in fake_output])\n",
    "\n",
    "# Confusion Matrix and Metrics\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "generator.save('D:\\Projects\\CDP\\Model\\gan_generator_model.h5')\n",
    "discriminator.save('D:\\Projects\\CDP\\Model\\gan_discriminator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "The image is predicted as REAL.\n"
     ]
    }
   ],
   "source": [
    "# Predict an image\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the trained generator and discriminator models\n",
    "generator = tf.keras.models.load_model('D:\\Projects\\CDP\\Model\\gan_generator_model.h5')\n",
    "discriminator = tf.keras.models.load_model('D:\\Projects\\CDP\\Model\\gan_discriminator_model.h5')\n",
    "\n",
    "# Normalize the image to [-1, 1] as we did during training\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(28, 28), color_mode=\"grayscale\")\n",
    "    img = image.img_to_array(img)\n",
    "    img = (img - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "# Function to predict if an image is real or fake\n",
    "def predict_real_or_fake(img_path, label):\n",
    "    img = preprocess_image(img_path)\n",
    "\n",
    "    # Add the label as a batch of size 1 (e.g., label 0 or 1)\n",
    "    label = np.array([label]).reshape(-1, 1)\n",
    "    \n",
    "    # Pass the image and label to the discriminator\n",
    "    prediction = discriminator([img, label], training=False)\n",
    "\n",
    "    # If prediction > 0.5, it predicts \"real\"; otherwise, \"fake\"\n",
    "    if prediction > 0:\n",
    "        print(\"The image is predicted as REAL.\")\n",
    "    else:\n",
    "        print(\"The image is predicted as FAKE.\")\n",
    "\n",
    "# Example usage\n",
    "sample_image_path = 'D:/Projects/CDP/fake-image.webp'  # Replace with the path to your sample image\n",
    "sample_label = 1  # Label associated with the real image (change accordingly)\n",
    "predict_real_or_fake(sample_image_path, sample_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
